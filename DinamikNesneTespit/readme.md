 dynamicdetection2 
 ---livemode.py ( live mod )
 --- dynamicdetection2.py (ror kullanÄ±lan yeni versiyon)


---

# Robotaxi Perception System

Bu proje, **LiDAR**, **IMU** ve **kamera** verilerini kullanarak hareketli ve statik nesneleri algÄ±layan, takip eden ve sÄ±nÄ±flandÄ±ran bir ROS 2 uyumlu **Perception Sistemi** iÃ§erir. Kod hem **test modunda** hem de **live modunda** Ã§alÄ±ÅŸacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.

---

## Genel BakÄ±ÅŸ

* **Test Modu:** KayÄ±tlÄ± sensÃ¶r verileri Ã¼zerinde Ã§alÄ±ÅŸÄ±r. LiDAR, IMU ve kamera kayÄ±tlarÄ±nÄ± senkronize ederek nesne tespiti, takip ve sÄ±nÄ±flandÄ±rma yapar. SonuÃ§larÄ± CSV ve JSON dosyalarÄ±na yazar. AyrÄ±ca gÃ¶rÃ¼ntÃ¼ Ã¼zerinde tespitleri gÃ¶rselleÅŸtirir.

* **Live Modu:** GerÃ§ek zamanlÄ± olarak ROS 2 sensÃ¶r topiclerinden veri alÄ±r (IMU, LiDAR, kamera). Nesneleri algÄ±lar ve takip eder, Ã§Ä±ktÄ±yÄ± dosyaya kaydeder. SÃ¼rekli Ã§alÄ±ÅŸÄ±r, elle durdurulana kadar aÃ§Ä±k kalÄ±r.

---

## KullanÄ±m

```bash
python robotaxi_perception_final.py --mode test
# veya
python robotaxi_perception_final.py --mode live
```

---

## Modlar DetaylarÄ±

### Test Modu

* **Girdi:**

  * IMU verisi: CSV dosyasÄ± (Ã¶rnek: `sensor_abs_data.csv`), iÃ§inde `timestamp`, `accel_x`, `accel_y` gibi sÃ¼tunlar var.
  * LiDAR verileri: PLY formatÄ±nda point cloud dosyalarÄ± (`lidar` klasÃ¶rÃ¼).
  * Kamera gÃ¶rÃ¼ntÃ¼leri: PNG formatÄ±nda (`camera` klasÃ¶rÃ¼).

* **Ä°ÅŸlem:**

  * Her frame iÃ§in kamera gÃ¶rÃ¼ntÃ¼sÃ¼ Ã¼zerinde YOLOv5 ile nesne algÄ±lama yapÄ±lÄ±r.
  * AlgÄ±lanan nesnelerin gÃ¶rÃ¼ntÃ¼ koordinatlarÄ± ile LiDAR noktalarÄ± eÅŸleÅŸtirilir (en yakÄ±n nokta bulunur).
  * IMU verisinden araÃ§ ivmesi hesaplanÄ±r, bu ivme nesnelerin hÄ±zÄ±nÄ± tahmin etmekte kullanÄ±lÄ±r.
  * Unscented Kalman Filter (UKF) ile nesne pozisyonlarÄ± takip edilir.
  * Statik ve dinamik nesneler hÄ±z eÅŸiklerine gÃ¶re sÄ±nÄ±flandÄ±rÄ±lÄ±r.
  * SonuÃ§lar hem CSV hem JSON olarak kaydedilir.
  * Kamera gÃ¶rÃ¼ntÃ¼sÃ¼nde tespitler gÃ¶sterilir.

* **Ã‡Ä±ktÄ±:**

  * `output/test_tracking_output.csv`
  * `output/test_tracking_output.json`

### Live Modu

* **Girdi:**

  * ROS 2 Ã¼zerinden gerÃ§ek zamanlÄ± sensÃ¶r topicleri:

    * `/imu/data` (IMU)
    * `/velodyne_points` (LiDAR)
    * `/camera/image_raw` (Kamera)

* **Ä°ÅŸlem:**

  * Test modundaki iÅŸleyiÅŸin aynÄ±sÄ±, ancak sensÃ¶r verileri gerÃ§ek zamanlÄ± olarak alÄ±nÄ±r.
  * UKF ile nesne pozisyonlarÄ± takip edilir ve hareketli/durgun ayrÄ±mÄ± yapÄ±lÄ±r.
  * Her yeni kamera gÃ¶rÃ¼ntÃ¼sÃ¼nde nesne takibi gÃ¼ncellenir.
  * Ã‡Ä±ktÄ±lar anlÄ±k CSV dosyasÄ±na kaydedilir.

* **Ã‡Ä±ktÄ±:**

  * `output/live_output.csv`

---

## Ã–nemli Parametreler

| Parametre            | AÃ§Ä±klama                                     | Optimal DeÄŸer | DeÄŸer Artarsa                            | DeÄŸer AzalÄ±rsa                                       |
| -------------------- | -------------------------------------------- | ------------- | ---------------------------------------- | ---------------------------------------------------- |
| `FRAME_RATE`         | Sistem kare/saniye oranÄ±                     | 20 FPS        | Daha hÄ±zlÄ± ama iÅŸlem yÃ¼kÃ¼ artar          | Daha yavaÅŸ, gecikme olabilir                         |
| `MAX_MATCH_DISTANCE` | Takip iÃ§in eÅŸleÅŸtirme mesafe eÅŸiÄŸi (m)       | 2.5 m         | Ã‡ok bÃ¼yÃ¼k olursa yanlÄ±ÅŸ eÅŸleÅŸmeler artar | Ã‡ok kÃ¼Ã§Ã¼k olursa takip zorlaÅŸÄ±r, yeni IDâ€™ler Ã§oÄŸalÄ±r |
| `model.conf`         | YOLOv5 algÄ±lama gÃ¼ven eÅŸiÄŸi                  | 0.3           | DÃ¼ÅŸÃ¼kse yanlÄ±ÅŸ pozitif artar             | Ã‡ok yÃ¼ksekse gerÃ§ek nesneler gÃ¶zÃ¼kmeyebilir          |
| `UKF_TRACK_LIMIT`    | AynÄ± anda takip edilen maksimum nesne sayÄ±sÄ± | 30 nesne      | Bellek ve iÅŸlem yÃ¼kÃ¼ artar               | Takip kaybÄ± ve algÄ±lamada yetersizlik                |

---

## Kodun Temel Ä°ÅŸleyiÅŸi

* **Nesne algÄ±lama:** Kamera gÃ¶rÃ¼ntÃ¼sÃ¼ne YOLOv5 uygulanÄ±r.
* **LiDAR temizleme:** Zemin altÄ± noktalar Ã§Ä±karÄ±lÄ±r, gÃ¼rÃ¼ltÃ¼ azaltÄ±lÄ±r.
* **Nesne-LiDAR eÅŸleÅŸtirme:** GÃ¶rÃ¼ntÃ¼deki nesneler ile LiDAR noktalarÄ± mesafeye gÃ¶re eÅŸleÅŸtirilir.
* **Hareketlilik tahmini:** IMUâ€™dan araÃ§ ivmesi alÄ±nÄ±r, nesnelerin hÄ±z tahmini yapÄ±lÄ±r.
* **Takip:** Unscented Kalman Filter kullanÄ±larak nesne pozisyonlarÄ± stabilize edilir.
* **SÄ±nÄ±flandÄ±rma:** Statik nesneler listesi ve hÄ±z eÅŸiklerine gÃ¶re hareketli/ statik ayrÄ±mÄ± yapÄ±lÄ±r.
* **Ã‡Ä±ktÄ± kaydÄ±:** Zaman damgasÄ±, nesne pozisyonu, mesafe ve hareketlilik tÃ¼rÃ¼ CSV ve JSONâ€™a yazÄ±lÄ±r.

---

## Gereksinimler

* Python 3.7+
* PyTorch (YOLOv5 iÃ§in)
* Open3D
* OpenCV
* FilterPy
* SciPy
* Pandas
* ROS 2 (live mod iÃ§in)
* CvBridge, sensor\_msgs, rclpy (live mod iÃ§in)

---

## Notlar

* `STATIC_CLASSES` listesine eklenen nesneler her zaman statik kabul edilir (Ã¶rneÄŸin "kite" etiketi).
* Kod, LiDAR noktasÄ±nÄ±n gerÃ§ek konumunu doÄŸrudan kullanÄ±r; aracÄ±n hareketi IMU ivmesiyle yaklaÅŸÄ±k olarak tahmin edilir.
* Test modu kolayca kendi verilerinizle deneyip Ã§Ä±ktÄ± almanÄ±zÄ± saÄŸlar.
* Live modda gerÃ§ek sensÃ¶r verileri ile ROS 2 altyapÄ±sÄ±nda Ã§alÄ±ÅŸÄ±r.

---

## Ä°letiÅŸim

Kodla ilgili sorularÄ±n veya Ã¶nerilerin iÃ§in bana ulaÅŸabilirsin. Kolay gelsin! ğŸš—ğŸ¤–
